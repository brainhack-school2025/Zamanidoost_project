{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17aca12c-2ccb-4549-babc-fecef78f808d",
   "metadata": {},
   "source": [
    "## Step 7: Load Training Data from `.pkl` File\n",
    "\n",
    "This step loads the preprocessed data prepared in the previous step:\n",
    "\n",
    "- **Inputs:** A `.pkl` file containing:\n",
    "  - `images`: The original 2D slices (typically shape: N × H × W × 3).\n",
    "  - `GT`: Ground truth bounding boxes.\n",
    "  - `reg`: Regression targets for bounding box localization.\n",
    "  - `clc`: Classification labels (1: object, 0: background, -1: ignore).\n",
    "- **Conversion:** Data is stored in NumPy arrays and ready for input into a PyTorch model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ba09e7-3cc4-4e16-8f46-2325db585eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('./Brainhack/final_project/data_train.pkl', 'rb') as f:\n",
    "    Data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "image_data = Data['images']\n",
    "bbox_list1 = Data['GT']\n",
    "offset_list_label_list1 = Data['reg']\n",
    "label_list1 = Data['clc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbdbe28-f0a2-467e-8ec4-f2ea1df6ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca0ed0-ed10-4ae5-8471-5d4e8e5b5662",
   "metadata": {},
   "source": [
    "## Step 8: Define the Custom RPN Architecture Based on VGG16\n",
    "\n",
    "This step constructs the RPN model used for region proposal generation:\n",
    "\n",
    "- **Backbone:** A custom VGG16-like convolutional architecture is implemented.\n",
    "- **Upsampling:** A transpose convolution layer is added to refine spatial resolution.\n",
    "- **Heads:**\n",
    "  - **Regression head**: Predicts 4 × *k* values (bounding box offsets).\n",
    "  - **Classification head**: Predicts objectness score for each anchor (binary classification).\n",
    "- **Customization:** Additional conv layers allow concatenation of multi-scale features to improve proposal quality.\n",
    "\n",
    "## Step 9: Load Pretrained Weights from VGG16\n",
    "\n",
    "This step initializes the convolutional layers of the custom RPN with weights from a pretrained VGG16 model:\n",
    "\n",
    "- **Transfer Learning:** Weights are copied layer-by-layer from `torchvision.models.vgg16`.\n",
    "- **Selective Copying:** Only matching layers of type `nn.Conv2d` are transferred.\n",
    "- **Benefits:** Faster convergence and better initial performance on medical image data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5665ee3-6cf2-473e-a8e7-a2f9e5e2176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "            Conv2d-2         [-1, 64, 512, 512]          36,928\n",
      "         MaxPool2d-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4        [-1, 128, 256, 256]          73,856\n",
      "            Conv2d-5        [-1, 128, 256, 256]         147,584\n",
      "         MaxPool2d-6        [-1, 128, 128, 128]               0\n",
      "            Conv2d-7        [-1, 256, 128, 128]         295,168\n",
      "            Conv2d-8        [-1, 256, 128, 128]         590,080\n",
      "            Conv2d-9        [-1, 256, 128, 128]         590,080\n",
      "        MaxPool2d-10          [-1, 256, 64, 64]               0\n",
      "           Conv2d-11          [-1, 512, 64, 64]       1,180,160\n",
      "           Conv2d-12          [-1, 512, 64, 64]       2,359,808\n",
      "           Conv2d-13          [-1, 512, 64, 64]       2,359,808\n",
      "        MaxPool2d-14          [-1, 512, 32, 32]               0\n",
      "           Conv2d-15          [-1, 512, 32, 32]       2,359,808\n",
      "           Conv2d-16          [-1, 512, 32, 32]       2,359,808\n",
      "           Conv2d-17          [-1, 512, 32, 32]       2,359,808\n",
      "  ConvTranspose2d-18          [-1, 512, 62, 62]         262,656\n",
      "           Conv2d-19            [-1, 1, 64, 64]             513\n",
      "           Conv2d-20            [-1, 1, 64, 64]             257\n",
      "           Conv2d-21          [-1, 510, 64, 64]         261,630\n",
      "           Conv2d-22           [-1, 24, 64, 64]          12,312\n",
      "           Conv2d-23            [-1, 6, 64, 64]           3,078\n",
      "================================================================\n",
      "Total params: 15,255,134\n",
      "Trainable params: 15,255,134\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 631.95\n",
      "Params size (MB): 58.19\n",
      "Estimated Total Size (MB): 693.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the RPN model in PyTorch\n",
    "class CustomRPN(nn.Module):\n",
    "    def __init__(self, k=6, weight_decay=0.000001):\n",
    "        super(CustomRPN, self).__init__()\n",
    "\n",
    "        # Define the initial convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Transpose convolution to upsample feature maps\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=1, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # Convolutional layers to adjust channel dimensions\n",
    "        self.conv15 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, padding=1)\n",
    "        self.conv16 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n",
    "        self.conv17 = nn.Conv2d(in_channels=512, out_channels=510, kernel_size=1)\n",
    "        \n",
    "        # Define regressor and classifier layers\n",
    "        self.regressor = nn.Conv2d(in_channels=512, out_channels=4*k, kernel_size=1)\n",
    "        self.classifier = nn.Conv2d(in_channels=512, out_channels=k, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x1))\n",
    "        p1 = self.pool(x2)\n",
    "        x3 = F.relu(self.conv3(p1))\n",
    "        x4 = F.relu(self.conv4(x3))\n",
    "        p2 = self.pool(x4)\n",
    "        x5 = F.relu(self.conv5(p2))\n",
    "        x6 = F.relu(self.conv6(x5))\n",
    "        x7 = F.relu(self.conv7(x6))\n",
    "        p3 = self.pool(x7)\n",
    "        x8 = F.relu(self.conv8(p3))\n",
    "        x9 = F.relu(self.conv9(x8))\n",
    "        x10 = F.relu(self.conv10(x9))\n",
    "        p4 = self.pool(x10)\n",
    "        x11 = F.relu(self.conv11(p4))\n",
    "        x12 = F.relu(self.conv12(x11))\n",
    "        x13 = F.relu(self.conv13(x12))\n",
    "        \n",
    "        # Upsample the feature maps\n",
    "        x14 = self.deconv1(x13)\n",
    "        x15 = self.conv15(x14)\n",
    "        # Additional convolutions on earlier layers\n",
    "        x18 = self.conv16(p3)\n",
    "        x19 = self.conv17(x10)\n",
    "\n",
    "        \n",
    "        # Concatenate feature maps along the channel dimension\n",
    "        concatenated = torch.cat([x15, x19, x18], dim=1)\n",
    "        \n",
    "        # Get bounding box predictions and objectness scores\n",
    "        regressor = self.regressor(concatenated)\n",
    "        classifier = torch.sigmoid(self.classifier(concatenated))\n",
    "\n",
    "        return regressor, classifier\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomRPN(k=6)\n",
    "\n",
    "# Load pretrained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Function to load weights from VGG16 to the CustomRPN model\n",
    "def load_vgg16_weights(custom_model, vgg16_model):\n",
    "    vgg16_layers = list(vgg16_model.features.children())  # Get layers from VGG16\n",
    "    custom_layers = list(custom_model.children())  # Get layers from CustomRPN\n",
    "\n",
    "    # Transfer weights from VGG16 to CustomRPN\n",
    "    vgg_idx = 0\n",
    "    for custom_layer in custom_layers:\n",
    "        if isinstance(custom_layer, nn.Conv2d):\n",
    "            vgg_layer = vgg16_layers[vgg_idx]\n",
    "            if isinstance(vgg_layer, nn.Conv2d):\n",
    "                with torch.no_grad():\n",
    "                    custom_layer.weight.copy_(vgg_layer.weight)\n",
    "                    custom_layer.bias.copy_(vgg_layer.bias)\n",
    "                vgg_idx += 1\n",
    "\n",
    "# Load the weights into the model\n",
    "load_vgg16_weights(model, vgg16)\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebe7cc-9072-445a-8ee6-75ae5a4387e4",
   "metadata": {},
   "source": [
    "## Step 10: Define Custom Loss Functions\n",
    "\n",
    "This step defines the loss functions used for RPN training:\n",
    "\n",
    "- **Smooth L1 Loss:** Used for regression of bounding box offsets, robust to outliers.\n",
    "- **Custom L1 Loss:** Applies Smooth L1 loss only to positive anchors.\n",
    "- **Custom Binary Loss:** Binary cross-entropy loss applied to anchor objectness scores, ignoring neutral anchors.\n",
    "\n",
    "> These losses are combined during training to optimize both localization and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e56a7b7-275d-40c6-bba2-caca75ec5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmoothL1Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Calculate the absolute difference between true and predicted values\n",
    "        x = torch.abs(y_true - y_pred)\n",
    "        # Create a mask for values where the absolute difference is less than 1\n",
    "        mask = (x < 1.0).float()\n",
    "        # Compute the Smooth L1 loss based on the mask\n",
    "        loss = mask * (0.5 * x ** 2) + (1 - mask) * (x - 0.5)\n",
    "        return loss.mean()\n",
    "\n",
    "class CustomL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomL1Loss, self).__init__()\n",
    "        # Initialize SmoothL1Loss as a component of CustomL1Loss\n",
    "        self.smooth_l1_loss = SmoothL1Loss()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Reshape y_pred to match the format of y_true\n",
    "        y_pred = y_pred.view(y_true.shape[0], y_true.shape[1], -1)  \n",
    "        # Extract bounding box offsets and labels from y_true\n",
    "        offset_list = y_true[:, :, :-1]\n",
    "        label_list = y_true[:, :, -1]\n",
    "        # Identify positive examples where the label is 1\n",
    "        positive_idxs = (label_list == 1).nonzero(as_tuple=True)\n",
    "        # Select the predicted bounding boxes and target offsets for positive examples\n",
    "        bbox = y_pred[positive_idxs]\n",
    "        target_bbox = offset_list[positive_idxs]\n",
    "        # Compute the Smooth L1 loss between the predicted and target bounding boxes\n",
    "        loss = self.smooth_l1_loss(target_bbox, bbox)\n",
    "        return loss\n",
    "\n",
    "class CustomBinaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBinaryLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred_objectiveness):\n",
    "        # Reshape predictions to match the expected shape\n",
    "        y_pred = y_pred_objectiveness.view(-1, 24576)\n",
    "        y_true = y_true.squeeze(-1)\n",
    "        # Get the indices where the true labels are not -1\n",
    "        indices = (y_true != -1).nonzero(as_tuple=True)\n",
    "        # Select the predicted logits and true labels for the valid indices\n",
    "        rpn_match_logits = y_pred[indices]\n",
    "        anchor_class = y_true[indices]\n",
    "        \n",
    "        # Clamp anchor_class values to be between 0 and 1\n",
    "        anchor_class = torch.clamp(anchor_class, min=0, max=1)  \n",
    "        \n",
    "        loss = F.binary_cross_entropy(rpn_match_logits, anchor_class.float()) # Make sure anchor_class is float\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b64877f6-f93c-497b-ae1d-4d67fe5d8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc1408-78be-4829-afe4-32d36dd3d35e",
   "metadata": {},
   "source": [
    "## Step 11: Prepare Data for Training\n",
    "\n",
    "This step converts and splits the data for PyTorch training:\n",
    "\n",
    "- **Format Conversion:** Image data is transposed to `(N, C, H, W)` and converted to float tensors.\n",
    "- **Splitting:** Data is split into training and validation sets (90/10).\n",
    "- **Dataloaders:** PyTorch `DataLoader` objects are created for batching and shuffling.\n",
    "\n",
    "## Step 12: Train and Validate the RPN Model\n",
    "\n",
    "This step performs training and validation of the RPN over multiple epochs:\n",
    "\n",
    "- **Optimizer:** Adam optimizer with a learning rate of 0.0001.\n",
    "- **Training Loop:** Runs for 5000 epochs (or until early stopping).\n",
    "- **Loss Composition:** Total loss = classification loss + 0.2 × regression loss.\n",
    "- **Validation:** Evaluates the model on validation set at each epoch.\n",
    "\n",
    "> Device used: CUDA (if available), otherwise CPU\n",
    "\n",
    "## Step 13: Save the Trained Model\n",
    "\n",
    "After training completes, the RPN model is saved for future use:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866e1006-3f95-4e0c-b98a-bbb4fbe3933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Average Loss: 1.0576949480614501\n",
      "Validation Loss: 1.0777614690911153\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Average Loss: 0.99862130030559\n",
      "Validation Loss: 0.9977648503878238\n",
      "------------------------------\n",
      "Epoch 3/10\n",
      "Average Loss: 0.9279586158982654\n",
      "Validation Loss: 0.9708523356212833\n",
      "------------------------------\n",
      "Epoch 4/10\n",
      "Average Loss: 0.9022415641102602\n",
      "Validation Loss: 0.9650765114224359\n",
      "------------------------------\n",
      "Epoch 5/10\n",
      "Average Loss: 0.9033681851915313\n",
      "Validation Loss: 0.9405174825867645\n",
      "------------------------------\n",
      "Epoch 6/10\n",
      "Average Loss: 0.8894290972435194\n",
      "Validation Loss: 0.9405467327471195\n",
      "------------------------------\n",
      "Epoch 7/10\n",
      "Average Loss: 0.8732365614180289\n",
      "Validation Loss: 0.9350577168737304\n",
      "------------------------------\n",
      "Epoch 8/10\n",
      "Average Loss: 0.8648839116651054\n",
      "Validation Loss: 0.9204126436941592\n",
      "------------------------------\n",
      "Epoch 9/10\n",
      "Average Loss: 0.8334147511162234\n",
      "Validation Loss: 0.8962395561751278\n",
      "------------------------------\n",
      "Epoch 10/10\n",
      "Average Loss: 0.820858475850294\n",
      "Validation Loss: 0.86716595122065\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Transpose the image data to match the PyTorch tensor format (N, C, H, W)\n",
    "image_data1 = np.transpose(image_data, (0, 3, 1, 2))\n",
    "\n",
    "# Convert image data and target labels to PyTorch tensors\n",
    "images = torch.tensor(image_data1)\n",
    "images = images.to(dtype=torch.float32)\n",
    "target_regressor = torch.tensor(offset_list_label_list1)\n",
    "target_classifier = torch.tensor(label_list1)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_images, val_images, train_target_regressor, val_target_regressor, train_target_classifier, val_target_classifier = train_test_split(\n",
    "    images, target_regressor, target_classifier, test_size=0.1)\n",
    "\n",
    "# Create DataLoader for training data with a batch size of 4\n",
    "train_dataset = TensorDataset(train_images, train_target_regressor, train_target_classifier)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "# Create DataLoader for validation data with a batch size of 4\n",
    "val_dataset = TensorDataset(val_images, val_target_regressor, val_target_classifier)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise CPU\n",
    "model = CustomRPN(k=6).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer with learning rate of 0.0001\n",
    "\n",
    "# Define the loss functions\n",
    "custom_l1_loss = CustomL1Loss()\n",
    "custom_binary_loss = CustomBinaryLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images, target_regressor, target_classifier = batch\n",
    "        images, target_regressor, target_classifier = images.to(device), target_regressor.to(device), target_classifier.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        regressor, classifier = model(images)\n",
    "\n",
    "        # Compute losses for regression and classification\n",
    "        loss_regressor = custom_l1_loss(target_regressor, regressor)\n",
    "        loss_classifier = custom_binary_loss(target_classifier, classifier)\n",
    "        loss = loss_classifier + 0.2 * loss_regressor  # Combine losses with a weight for regression loss\n",
    "\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)  # Compute average loss\n",
    "    print(f'Average Loss: {average_loss}')\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No gradient computation for validation\n",
    "        for batch in dataloader:\n",
    "            images, target_regressor, target_classifier = batch\n",
    "            images, target_regressor, target_classifier = images.to(device), target_regressor.to(device), target_classifier.to(device)\n",
    "            \n",
    "            regressor, classifier = model(images)\n",
    "\n",
    "            # Compute losses for regression and classification\n",
    "            loss_regressor = custom_l1_loss(target_regressor, regressor)\n",
    "            loss_classifier = custom_binary_loss(target_classifier, classifier)\n",
    "            loss = loss_classifier + 0.2 * loss_regressor  # Combine losses with a weight for regression loss\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)  # Compute average loss\n",
    "    print(f'Validation Loss: {average_loss}')\n",
    "\n",
    "# Set the number of epochs and batch size\n",
    "num_epochs = 5000  # Number of epochs for training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, train_dataloader, optimizer, device)  # Train for one epoch\n",
    "    validate(model, val_dataloader, device)  # Validate after training\n",
    "    print('-' * 30)\n",
    "\n",
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), \"RPN_torch_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8d1b8-0a1c-413a-8678-59b67e7d0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
